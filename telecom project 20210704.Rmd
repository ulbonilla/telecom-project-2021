---
title: "Telecommunications project"
author: "Ulises Bonilla"
date: "28/5/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, message = FALSE, warning = FALSE, echo = FALSE}

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE,
                      echo = FALSE)
rm(list=ls())

# the path directory contains all working files for this project
path <- "C:/Users/User/OneDrive/Documentos"  # change if needed
setwd(path)

#Sys.setlocale("LC_TIME", "English")

```



```{r libraries}
# libraries used in this project are authomatically installed
if (!require("readr")) install.packages("readr")
library("readr")
if (!require("kaggler")) install.packages("kaggler")
library("kaggler") # kaggle API
if (!require("tidyverse")) install.packages("tidyverse")
library("tidyverse")
if (!require("knitr")) install.packages("knitr")
library("knitr") # tables
if (!require("rlang")) install.packages("rlang")
library("rlang")
if (!require("sjPlot")) install.packages("sjPlot")
library("sjPlot") # chart grid
if (!require("caret")) install.packages("caret")
library("caret")
if (!require("randomForest")) install.packages("randomForest")
library("randomForest") # random forest
if (!require("naivebayes")) install.packages("naivebayes")
library("naivebayes") # naive bayes
if (!require("nnet")) install.packages("nnet")
library("nnet") # neural networks
if (!require("glmnet")) install.packages("glmnet")
library("glmnet") # logistic
if (!require("Hmisc") ) install.packages("Hmisc")
library("Hmisc") # for correlation table

```


## Introduction ##

Any business wants to maximize the number of customers. To achieve this goal, it is important not only to try to attract new ones, but also to retain existing ones. Retaining a client will cost the company less than attracting a new one. In addition, a new client may be weakly interested in business services and it will be difficult to work with him, while old clients already have the necessary data on interaction with the service.

The Churn rate (sometimes called attrition rate), in its broadest sense, is a measure of the number of individuals or items moving out of a collective group over a specific period. Accordingly, by predicting the churn, a firm can react in time and try to keep the client who wants to leave. Based on the data about the services that the client uses, the firm can make him a special offer, trying to change his decision to leave the operator. This will make the task of retention easier to implement than the task of attracting new users, about which the firm does not know anything yet.

### The data ###


The `Telecom users dataset` provided by `Radmir Zosimov`is publicly available at https://www.kaggle.com/radmirzosimov/telecom-users-dataset. The data contains information about almost six thousand users, their demographic characteristics, the services they use, the duration of using the operator's services, the method of payment, and the amount of payment. The data can be either downloaded manually or using Kaggle´s API. I proceed with the latter.

```{r downloading data from API, eval = FALSE}

# this code chuck has to be evaluated only once

key <- str_c(path,'/kaggle.json') # kaggle API key

if (file.exists(key)) {
  
  kaggler::kgl_auth(creds_file = key) # to run this code you need a kaggle key!!
  
  response <- kgl_datasets_download_all(owner_dataset = "radmirzosimov/telecom-users-dataset")
  
  download.file(response[["url"]],  str_c(path,"/data.zip"), mode="wb")
  
  unzip(str_c(path,"/data.zip"), exdir = str_c(path,"/data"), overwrite = TRUE)
  
  rm(response) # not used
  
  data <- read.csv(str_c(path,"/data/telecom_users.csv"), 
                   stringsAsFactors = TRUE) %>% 
          select(-X)
  
} else { # without the key, the data can be downloaded from github
  
  data <- read.csv("https://raw.githubusercontent.com/ulbonilla/telecom-project-2021/main/telecom_users.csv", 
          stringsAsFactors = TRUE) %>% 
          select(-X)

}

```


```{r reading the data}
# read downloaded data into r
data <- read.csv(str_c(path,"/data/telecom_users.csv"), 
                 stringsAsFactors = TRUE) %>% 
        select(-X)

```

The data contains `r dim(data)[1]` rows and `r dim(data)[2]` columns. According to the information provided by the source, the columns are:

1. `customerID` - customer id
2. `gender` - client gender (male / female)
3. `SeniorCitizen` - is the client retired (1, 0)
4. `Partner` - is the client married (Yes, No)
5. `tenure` - how many months a person has been a client of the company
6. `PhoneService` - is the telephone service connected (Yes, No)
7. `MultipleLines` - are multiple phone lines connected (Yes, No, No phone service)
8. `InternetService` - client's Internet service provider (DSL, Fiber optic, No)
9. `OnlineSecurity` - is the online security service connected (Yes, No, No internet service)
10. `OnlineBackup` - is the online backup service activated (Yes, No, No internet service)
11. `DeviceProtection` - does the client have equipment insurance (Yes, No, No internet service)
12. `TechSupport` - is the technical support service connected (Yes, No, No internet service)
13. `StreamingTV` - is the streaming TV service connected (Yes, No, No internet service)
14. `StreamingMovies` - is the streaming cinema service activated (Yes, No, No internet service)
15. `Contract` - type of customer contract (Month-to-month, One year, Two year)
16. `PaperlessBilling` - whether the client uses paperless billing (Yes, No)
17. `PaymentMethod` - payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))
18. `MonthlyCharges` - current monthly payment
19. `TotalCharges` - the total amount that the client paid for the services for the entire time
20. `Churn` - whether there was a churn (Yes or No)

### Goal ###

Given these variables, the goal of this project is to predict the variable `Churn`, that is, whether the client remains or not. 

## Analysis ##

### Data Cleaning and exploration ###

The following first table summarizes the class of each column, as well as how many NAs each contains and how many unique values are there.

```{r table 1}
# table 1: class and number of NAs by column
table1 <- data %>% 
          lapply(function(x) class(x)) %>% # class of columns
          as.data.frame() %>% 
          gather(key = "column_name", value = "class") %>% 
          bind_cols(sapply(data, function(x) sum(is.na(x))) ) %>% 
          bind_cols(sapply(data, function(x) dim(data)[1] - sum(duplicated(x)))) %>% 
          set_names("column_name", "class", "NAs", "unique_values") 

knitr::kable(table1, # knitr tables have a better format
             align = "ccc", 
             col.names = c("Column name", 
                           "Class", 
                           "Number of NAs", 
                           "Number of unique values"))
```

As observed in the table above, there are `r sum(table1$class %in% c("integer", "numeric"))` columns with numeric values and `r sum(table1$class == "factor")` columns with character values. The character columns are categorical variables or factors. However, it is evident that the column `SeniorCitizen` is a categorical value as well because it has only `r table1$unique_values[which(table1$column_name=="SeniorCitizen")]` unique values. Moreover, notice that the column `Churn`, which identifies whether there was a churn or not, is categorical. Finally, notice that only the column `TotalCharges` contains NAs. So, i first have to apply a few changes to the data.  

Recall that the variable `TotalCharges` contains `r table1$NAs[[which(table1$column_name=="TotalCharges")]]` NAs which represents only the `r format(round( table1$NAs[[which(table1$column_name=="TotalCharges")]]/dim(data)[1]*100, 2), nsmall =2)`% of the total data. Yet, it may be worth it to review them. In this regard, it is noticed that the clients whose `TotalCharges` are NAs are also those for which their `tenure` is zero. Thus, the actual value should be zero, since they have not paid a single charge yet. This is thus adjusted.

```{r NAs data}

# data1 contains the 10 rows with NAs to be inspected manually
data1 <- data %>% 
         filter(is.na(TotalCharges)) %>% 
         select(customerID, tenure, TotalCharges)

# SeniorCitizen is not a numerica variable, but a categorical one
data <- data %>% 
        mutate(TotalCharges = ifelse(is.na(TotalCharges), 0, TotalCharges),
               SeniorCitizen = factor(SeniorCitizen))

```

Next, i explore the numerical variables. The following diagrams are histograms of the numeric variables, excluding `SeniorCitizen`.

```{r graph 1}
# graph 1: historgrams of numeric variables
graph1 <-  data %>% 
           select_if(is.numeric) %>% 
           gather(key = "column", value = "value") %>%  
           mutate(column = ifelse(column == "tenure", "Tenure", column),
                  column = ifelse(column == "MonthlyCharges", "Monthly Charges", column),
                  column = ifelse(column == "TotalCharges", "Total Charges", column)) %>% 
           ggplot() + 
           geom_histogram(aes(x= value, fill = column), alpha=0.6) +
           facet_wrap(~column, scales = "free_x") +
           theme_bw() +
           theme(legend.position = "none") +
           xlab("") +
           ylab("") +
           ggtitle(label = "Histograms of numeric variables")

show(graph1)
```

```{r table2}
# table 2: statistics of numerical variables
table2 <- data %>% 
          select_if(is.numeric) %>% 
          gather(key = "column_name", value = "value") %>% 
          group_by(column_name) %>% 
          summarise(mean = format(round( mean(value, na.rm=TRUE),2), nsmall =2, big.mark = ","),
                    max = format(round( max(value, na.rm=TRUE),2), nsmall =2, big.mark = ","),
                    min = format(round( min(value, na.rm=TRUE),2), nsmall =2, big.mark = ","),
                    sd = format(round( sd(value, na.rm=TRUE),2), nsmall =2, big.mark = ",") ) 
  
knitr::kable(table2, 
             align = "ccc", 
             col.names = c("Column name", 
                           "Mean", 
                           "Maximum", 
                           "Minumum",
                           "Standard deviation"))
```

```{r table 3}
# table 3: correlation betwen numerical variables
table3 <- rcorr(as.matrix(data[,c("tenure", "MonthlyCharges", "TotalCharges")]))[[1]] %>% 
          as.data.frame()

knitr::kable(table3, 
             align = "ccc")
```

There are a few things to notice about these variables. First, all three are positive. Second, `TotalCharges` is the one with the highest volatility, measured in terms of standard deviation. Likewise, it is observed that the `tenure` variable has a mean number of months equal to `r table2$mean[which(table2$column_name=="tenure")]`, but its standard deviation is `r table2$sd[which(table2$column_name=="tenure")]`. This means that the time of contract varies considerably. And, third, it is also observed that `TotalCharges` is highly correlated with the other two variables, specially `tenure` (`r table3[3,1]`). These all makes sense because the total payments are higher the longer the tenure, there is not only correlation, but causality. 

The problem with these three variables is that they do not necessarily provide an accurate overview of the data.  Because `TotalCharges` highly depends on `tenure`, the former is also misleading. And lastly, `MonthlyCharges` is also misleading since it captures the payment due on the last month of the data only. It does not necessarily captures changes in payment levels across the tenure. Thus, it may be appropriate to use a better variable, such as `MeanCharges`, which is the average of  `TotalCharges` over the `tenure`.

To illustrate the latter, i present the following graph, which contains the density of the mean charges. It is observed that the new variable´s histogram is similar to the histogram of `MonthlyCharges`, since there are two main humps in the distribution. These could suggest a clear differentiation across clients, which could help to improve the predictability power of certain models.

```{r graph 2}
# graph 2: density of mean charges
graph2 <-  data %>% 
           summarise(MeanCharges = TotalCharges/tenure)  %>% 
           ggplot() + 
           geom_density(aes(x= MeanCharges),  fill="skyblue") +
           theme_bw() +
           theme(legend.position = "none") +
           xlab("") +
           ylab("") +
           ggtitle(label = "Density of mean charges")

show(graph2)

# MeanCharges will be used instead of TotalCharges and MonthlyCharges
data <- data %>% 
        mutate(MeanCharges = ifelse(tenure==0, 0, TotalCharges/tenure)) %>% 
        select(-TotalCharges, -MonthlyCharges)

```

```{r pie function}

# this function is used to create pie charts
pie_fun <- function(col_name, data){
  
  data %>% 
  group_by(!!sym(col_name)) %>% 
  summarise(porcentaje=n()/dim(data)[1],
            porcentaje_f = format(round(porcentaje*100,2),2)) %>% 
  ggplot(aes(x = "",y= porcentaje, fill = !!sym(col_name)))  +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start=0) + 
  geom_text(aes(label=porcentaje_f), 
            color = "white", 
            size=2, 
            position = position_stack(vjust = 0.5)) +
  xlab("Porcentaje (%)") +
  ylab("") +
  labs(fill = "") +
  ggtitle(label = str_c("Pie chart of column ", col_name)) +
  theme(text = element_text(size=6),
        legend.position = "bottom")
  
}
```

Next, i explore the rest of the variables, which are all categorical, including `Churn`. The following collection of pie charts show the categories present in each variable and their proportions.

```{r graph 3, eval = FALSE}
# names of categorical columns
names_f <- names(select_if(data, is.factor))[-c(1, 17)]

pie_charts_grid <- plot_grid(map(names_f, pie_fun, data), margin = c(.1, .1, .1, .1), tags = NULL)

ggsave(plot=pie_charts_grid  ,paste0(path, "/pie_charts_grid.png"), width = 8, height = 12)

```

![](//MyCloudEX2Ultra/HDulises/backup1/trabajo/IFT/cursos/EDX/harvard x/edx data science capstone/telecom/pie_charts_grid.png)

The most important of the pie charts above is the last one because it is the chart of the independent variable: `Churn`. In fact it is worth it to replicate such chart in a larger scale. 

```{r graph 4}
# Churn pie chart is replicated on its own
churn_d <-  data %>% 
            group_by(Churn) %>% 
            summarise(porcentaje=n()/dim(data)[1],
                      porcentaje_f = format(round(porcentaje*100,2),2)) 

pie_fun("Churn", data)

```

As observed above, `r churn_d$porcentaje_f[1]` of `Churn` are `No`. This means that the independent variable is unbalanced. An unbalanced dataset will bias the prediction model towards the more common class.

### Models´ implementation ###


Because the `Churn` column is categorical a classification supervised method would be appropriate to analyze this dataset. There are several ML models that I could use. I have chosen: Logistic regression, Naive bayes, Neural networks, Random forests, and SVM.

The first step for this analysis will be to split the data between training and test set. Afterwards, there are several candidate models of classification that are considered. These include: a. Support Vector Machines, b. Naive Bayes
c. Random Forest, d. Neural Networks, and e. Logistic regression. When needed, calibration and selection methods are also implemented. Finally, the evaluation of the models will be done comparing the confusion matrix and the accuracy of each of the models.

Before running the proposed models, there are a few pre-processing steps needed. First, I have to deal with the fact that most of the features are categorical variables. The `caret` package includes several functions to pre-process the predictor data. It assumes that all of the data are numeric, with the exception of the target variable `Chrun`, which should be a factor. Second, i have to split the data in a training and a testing set. And finally, third, I have to specify the train control settings that will be used in the models. The structure of the resulting data frame is the following:

```{r preprocessing}

# the first pre processing task is to convert all data to numeric data

y <- data %>% select(Churn)
x <- data[-1] %>% select(-Churn)

dummies <- caret::dummyVars(" ~ .", data = x)

# this is the data used for the models
data2 <- predict(dummies, newdata = x) %>% 
         data.frame() %>% 
         bind_cols(y)

utils::str(data2 , vec.len=4,width = 75, strict.width= "cut")

write.csv(data2, str_c(path, "/data2.csv"))

# the second task is to split the date into training and testing sets

index <- createDataPartition(data2$Churn, p = 0.8, list = FALSE)

train_data <- data2[index, ]
test_data  <- data2[-index, ]

write.csv(train_data, str_c(path, "/train_data.csv"))
write.csv(test_data, str_c(path, "/test_data.csv"))

# finally, i add the training settings

ctrl <- trainControl(method = "repeatedcv", 
                     number = 6, 
                     verboseIter = FALSE ,
                     classProbs =  TRUE,
                     repeats = 3)

```

```{r models, eval=FALSE}
# this code chunk has to be evaluated only once

train_data <- read.csv(str_c(path, "/train_data.csv")), 
                       stringsAsFactors = TRUE) %>% 
              select(-X)

# support vector machine
model_svm <- caret::train(Churn ~ .,
                            data = train_data,
                            method = "svmLinear",
                            preProcess = c("scale", "center"),
                            trControl = ctrl,
                            na.action = na.exclude)

saveRDS(object = model_svm, file = str_c(path, "/model_svm.rds"))

# naive bayes
model_nb <- caret::train(Churn ~ .,
                          data = train_data,
                          method = "naive_bayes",
                          preProcess = c("scale", "center"),
                          trControl = ctrl,
                          na.action = na.exclude)

saveRDS(object = model_nb, file = str_c(path, "/model_nb.rds"))

# random forests
model_rf <- caret::train(Churn ~ . ,
                         data = train_data,
                         method = "rf",
                         preProcess = c("scale", "center"),
                         trControl = ctrl,
                         na.action = na.exclude)

saveRDS(object = model_rf, file = str_c(path, "/model_rf.rds"))

# neural networks
model_nn <- caret::train(Churn ~ . ,
                         data = train_data,
                         method = "nnet",
                         preProcess = c("scale", "center"),
                         trControl = ctrl,
                         na.action = na.exclude,
                         MaxNWts=500,
                         maxit = 100, 
                         linout = FALSE)

saveRDS(object = model_nn, file = str_c(path, "/model_nn.rds"))

# logistic regression
model_lr <- caret::train(Churn ~ . ,
                         data = train_data,
                         method = 'glmnet',
                         preProcess = c("scale", "center"),
                         trControl = ctrl,
                         na.action = na.exclude,
                         family = 'binomial')

saveRDS(object = model_lr, file = str_c(path, "/model_lr.rds"))

```

The first model i implement is a linear support vector machine (svm). This is a relatively simple model in which no parameter is tuned.The trade of parameter is by default set at `C=1`.[^1]

```{r Support Vector Machine}

model_svm <- read_rds(str_c(path, "/model_svm.rds"))

model_svm


```

It is noted that, despite the simplicity of this first model, the results obtained from resampling are promising. The resampling estimation of accuracy in this model is `r model_svm$results[[2]]`.[^2] A second metric shown above is the kappa coefficient, which in this case equals `r model_svm$results[[3]]`.[^3]

The second model I use is naive bayes. Naive Bayes is a classification model that assumes that the effect of a particular feature in a class is independent of other variables. This is also a simple model in which its parameters are also kept constant. It can be observed that both accuracy and kappa are lower than the previous model. So, i do not expect this model to be the best performer with the testing data.

```{r Naive Bayes}

model_nb <- read_rds(str_c(path, "/model_nb.rds"))

model_nb

```

The third model I use is random forest. 

```{r Random Forest}

model_rf <- read_rds(str_c(path, "/model_rf.rds"))

model_rf

```

As observed in the results above, the best model is the one that makes use of `r model_rf$results[[1]][2]` variables, with an accuracy and a kappa of `r model_rf$results[[2]][2]` and `r model_rf$results[[3]][2]`, respectively. The graph below represents this same outcome:

```{r graph 5}

plot(model_rf)

```

The fourth model in the repertoire is neural networks. The results of this model are presented below:

```{r Neural Networks}

model_nn <- read_rds(str_c(path, "/model_nn.rds"))

model_nn

```
As observed, the best model is the one with size = 1 and decay = 0.1, which has an accuracy of `r model_nn$results[[3]][3]` and a kappa of `r model_nn$results[[4]][3]`. Graphically, this is represented below:

```{r graph 6}

plot(model_nn)

```

Finally, the fourth and last model presented here is the logistic regression. The logistic regression is a classification algorithm that predicts the probability of a categorical dependent variable (`Churn`). For our case, the goal of this model is to predict `P(Y=1)` as a function of the predictors.The fact that the best neural network above resulted in a single hidden unit suggests that a simpler model such as logistic could yield a good performance.

```{r Logistic regression }

model_lr <- read_rds(str_c(path, "/model_lr.rds"))

model_lr

```
As observed, the best performing model, that of accuracy `r model_lr$results[[3]][2]` and kappa `r model_lr$results[[4]][2]`, is one of the best performing among all models considered here.

```{r graph 7}

plot(model_lr)

```

## Results ##


Even though the resampling methods from the previous section provide an idea of the performance of the models, these still have to be tested against unseen data. In this section i compute several metrics resulting from using the models to predict the `Churn`, in the test data set.

```{r results}

models_files <- c("model_rf",
                  "model_svm",
                  "model_nb",
                  "model_lr",
                  "model_nn")

models_names <- c("Random Forest", 
                  "Support Vector Machine", 
                  "Naive Bayes", 
                  "Logistic Regression",
                  "Neural Network")

accuracy <- list()
other_metrics <- list()

# loop to extract the metrics from all models
for (i in 1:length(models_files)) {
  
  model <- readRDS(str_c(path, "/", models_files[i] ,".rds"))

  test_pred <- predict(model, newdata = test_data)

  accuracy[[i]] <- confusionMatrix(table(test_pred, test_data$Churn))[[3]]

  other_metrics[[i]] <- confusionMatrix(table(test_pred, test_data$Churn))[[4]]
  
}

# confusionMatrix computes two sets of metrics which i organize into 2 tables
accuracy <- bind_rows(lapply(accuracy, as.data.frame.list)) %>% 
            mutate(Model = models_names) %>% 
            gather("Metric", "Value", -Model) 

other_metrics <- bind_rows(lapply(other_metrics, as.data.frame.list)) %>% 
                 mutate(Model = models_names) %>% 
                 gather("Metric", "Value", -Model)

```

The first table of results provide traditional metrics, including accuracy and kappa, which I have addressed before. 

```{r table4}

table4 <- accuracy %>% 
          mutate(Value = format(round( Value, 4), nsmall =4, big.mark = ",") )  %>% 
          spread(Model, Value)

knitr::kable(table4, 
             align = "ccc")

```

As observed, the model with the highest accuracy is Random Forest, with `r table4[1,5]`, which is even higher than the resampling accuracy estimated before. Likewise, Random Forest also has the highest kappa coefficient with `r table4[6,5]`. 

However, as mentioned before, the data is unbalanced and this fact could affect metrics such as accuracy and kappa. For this reason, other metrics are also considered in the following table:

```{r table5}

table5 <- other_metrics %>% 
          mutate(Value = format(round( Value, 4), nsmall =4, big.mark = ",") )  %>% 
          spread(Model, Value)

knitr::kable(table5, 
             align = "ccc")

```

In particular, the two metrics commonly referred in the presence of unbalance data are the balanced accuracy and the F1 rate. As observed, both metrics coincide in that, once again, the Random forest model has the best performance. This model has the highest balanced accuracy with `r table5[1,5]` and the highest f1 rate with `r table5[4,5]`.

## Conclusions ##

- I used four different ML models, Logistic regression, Naive bayes, Neural networks, Random forests, and SVM, to predict the variable `Churn` from a telecom data base found online. According to my results, the best choice is the Random forest model because it shows a better performance accross different metrics.

- Potentially, this type of exercise can be extended to other markets, besides telecom, and generate great value for the firms. 

- The main limitation that i encountered was the size of the data. Now a days, a sample size such as the one that i used may not be considered robust enough. Because of this, future work would likely focus on improving the models by making use of larger data sets. 


[^1]:  It determines the possible misclassifications. It essentially imposes a penalty to the model for making an error: the higher the value of C, the less likely it is that the SVM algorithm will misclassify a point.
[^2]: Accuracy is simply a ratio of correctly predicted observation to the total observations.
[^3]: Cohen's kappa coefficient is a statistic that is used to measure inter-rater reliability (and also intra-rater reliability) for qualitative (categorical) items
